{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descrição do código\n",
    "\n",
    "Este código realiza o web scraping dos discursos realizados na Câmara dos Deputados.\n",
    "\n",
    "Link: https://www2.camara.leg.br/atividade-legislativa/discursos-e-notas-taquigraficas\n",
    "\n",
    "Por fim é gerado um arquivo em CSV, contendo o dicursos, o nome do orador, a sessão em que foi realizado o discursos, a fase em que foi realizado o discurso, a data, o partido do orador e o link para acessar o discurso na íntegra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pacotes Utilizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código utilizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df, content):    \n",
    "    new_content = {'Discurso': content.find(align=\"justify\").get_text(),\n",
    "                  'Sessão': re.search(re.compile(\"Sessão: ([\\w.]+)\"),\n",
    "                      content.find('td', text=re.compile('Sessão:')).get_text().strip()).group(1),\n",
    "                  'Fase': re.search(re.compile(\"Fase: ([\\w.]+)\"),\n",
    "                      content.find('td', text=re.compile('Fase:')).get_text().strip()).group(1),\n",
    "                  'Data': re.search(re.compile(\"Data: ([\\w/]+)\"),\n",
    "                      content.find('td', text=re.compile('Data:')).get_text().strip()).group(1)}\n",
    "    \n",
    "    df = df.append(new_content, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def vazio(df):\n",
    "    null_content = {'Discurso': None,\n",
    "                  'Sessão': None,\n",
    "                  'Fase': None,\n",
    "                  'Data': None}\n",
    "    df = df.append(null_content, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # Armazena discursos do site da câmara em um único dataframe.\n",
    "    #base_url = 'https://www.camara.leg.br/internet/sitaqweb/resultadoPesquisaDiscursos.asp?CurrentPage={page_number}&BasePesq=plenario&txIndexacao=&txOrador=&txPartido=&dtInicio=01/01/2017&dtFim=31/12/2017&txUF=&txSessao=&listaTipoSessao=&listaTipoInterv=&inFalaPres=&listaTipoFala=&listaFaseSessao=&txAparteante=&listaEtapa=&CampoOrdenacao=dtSessao&TipoOrdenacao=DESC&PageSize=50&txTexto=&txSumario='\n",
    "    base_url = 'https://www.camara.leg.br/internet/sitaqweb/resultadoPesquisaDiscursos.asp?txIndexacao=&CurrentPage={page_number}&BasePesq=plenario&txOrador=&txPartido=&dtInicio=01/01/2007&dtFim=31/12/2007&txUF=&txSessao=&listaTipoSessao=&listaTipoInterv=&inFalaPres=&listaTipoFala=&listaFaseSessao=&txAparteante=&listaEtapa=&CampoOrdenacao=dtSessao&TipoOrdenacao=DESC&PageSize=50&txTexto=&txSumario='\n",
    "    base_link = 'https://www.camara.leg.br/internet/sitaqweb/'\n",
    "    links = list()\n",
    "    oradores_tag = []\n",
    "    oradores=[]\n",
    "    partidos = []\n",
    "    pattern = re.compile('[A-ZÀÁÂĖÈÉÊÌÍÒÓÔÕÙÚÛÇ]+(?:[ ]|-|,|(?:das?|dos?|de|e|\\(|\\)|[A-ZÀÁÂÃĖÈÉÊÌÍÒÓÔÕÙÚÛÇ]+))*')\n",
    "    pattern_p = re.compile('[A-Z]+-(?:[A-Z]+)*')\n",
    "    df = pd.DataFrame({'Orador': [], 'Partido': [],'Discurso': [], 'Sessão': [], 'Data': [], 'Fase': [], 'Link': []})\n",
    "\n",
    "\n",
    "    \n",
    "    for page_number in range(1, 504):\n",
    "        # Para cada página, adiciona os links dos discursos em `links`\n",
    "        print(f'Obtendo links: página {page_number}', end='\\r')\n",
    "        site_data = requests.get(base_url.format(page_number=page_number))\n",
    "        soup = BeautifulSoup(site_data.content, 'html.parser')\n",
    "        link_tags = soup.find_all('a', href=re.compile('TextoHTML'))\n",
    "        for tag in link_tags:\n",
    "            links.append(re.sub(r\"\\s\", \"\", tag['href']))\n",
    "        tabela = soup.find('table', class_='table table-bordered variasColunas')\n",
    "        for row in tabela.findAll(\"tr\"): #para tudo que estiver em <tr>\n",
    "            cells = row.findAll('td') #variável para encontrar <td>\n",
    "            if len(cells)==8: #número de colunas\n",
    "                oradores_tag.append(cells[5].find(text=True)) #iterando sobre cada linha \n",
    "    for tag in oradores_tag:\n",
    "        oradores.append(str(pattern.findall(tag)))\n",
    "        partidos.append(str(pattern_p.findall(tag)))  \n",
    "    print()\n",
    "    \n",
    "    # Salva os links caso dê algum problema no caminho.\n",
    "    #with open('links_discursos_2013.txt', 'w') as f:\n",
    "     #   f.write('\\n'.join(links))\n",
    "        \n",
    "    n_links = len(links)\n",
    "    links_erro = list()\n",
    "    links_discursos = list()\n",
    "    print(f'Encontrados {n_links} links.')\n",
    "    print('Extraindo discursos...')\n",
    "    for n, link in enumerate(links):\n",
    "        link_data = requests.get(base_link+link)\n",
    "        content = BeautifulSoup(link_data.content, 'html.parser')\n",
    "        \n",
    "        if content:\n",
    "            try:\n",
    "                df = process(df, content)\n",
    "                links_discursos.append(base_link+link)\n",
    "            except:\n",
    "            # Salva os links com erro.\n",
    "                df = vazio(df)\n",
    "                links_erro.append(base_link+link)\n",
    "                links_discursos.append(base_link+link)\n",
    "                print(f'{n+1} discursos com erro.', end='\\r')\n",
    "        else:\n",
    "            df = vazio(df)\n",
    "            links_discursos.append(base_link+link)\n",
    "        print(f'{n+1} discursos de {n_links} extraídos.', end='\\r')\n",
    "    \n",
    "    with open('links_erro_2007.txt', 'w') as f:\n",
    "        f.write('\\n'.join(links_erro))\n",
    "    print()\n",
    "    df['Orador'] = oradores\n",
    "    df['Partido'] = partidos\n",
    "    df['Link'] = links_discursos\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = main()\n",
    "    df.to_csv('discursos\\camara_2007_novo.csv', index=False, sep=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82a3c348cc9fa6d74b560d52ce8ad16ae8cf45d1154fe3b3237f15db42a9d9a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
